{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fefd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "import tempfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a69351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-download-auto-examples-tutorials-run-word2vec-py\n",
    "def split(word):\n",
    "    return [char for char in word]\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        self.count = 0\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = \"only_ch.txt\"\n",
    "        for line in open(corpus_path, encoding='utf-8'):\n",
    "            # one document per line, tokens not separated\n",
    "            self.count += 1\n",
    "            if self.count % 50000 == 0:\n",
    "                end = time.time()\n",
    "                print(self.count)\n",
    "                print((end - self.start)/60) # in mins \n",
    "            #a =\"开 发了 即可 我 能 放\"\n",
    "            #print(utils.simple_preprocess(a))\n",
    "            #yield utils.simple_preprocess(line)\n",
    "            yield split(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403d7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633ba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "0.2589478890101115\n",
      "100000\n",
      "0.42760929266611736\n",
      "150000\n",
      "0.5699123422304789\n",
      "200000\n",
      "0.7028570969899496\n",
      "250000\n",
      "0.8304741700490316\n",
      "300000\n",
      "0.966543443997701\n",
      "350000\n",
      "1.090379520257314\n",
      "400000\n",
      "1.22016628185908\n",
      "450000\n",
      "1.8865447044372559\n",
      "500000\n",
      "2.6818228999773663\n",
      "550000\n",
      "3.3129217942555744\n",
      "600000\n",
      "3.931385171413422\n",
      "650000\n",
      "4.472918144861857\n",
      "700000\n",
      "5.0595053633054095\n",
      "750000\n",
      "5.629963751633962\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=sentences, \n",
    "    vector_size=150, \n",
    "    window=5, \n",
    "    min_count=1, \n",
    "    workers=4, \n",
    "    sg=0)\n",
    "model.save(\"pre_trained_char_150_iter5.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc74101",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model.wv['王']\n",
    "vec_king\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca822c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load \n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)\n",
    "    #\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    #\n",
    "    # To load a saved model:\n",
    "    #\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual result \n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d06a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7cad38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d51da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
